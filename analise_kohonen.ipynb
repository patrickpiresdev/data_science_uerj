{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "468e2ccf",
   "metadata": {},
   "source": [
    "# Kohonen - Iris\n",
    "\n",
    "- Aluno: Patrick Pires\n",
    "- Matricula: 201810037211\n",
    "\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy.ma.core import ceil\n",
    "from scipy.spatial import distance #distance calculation\n",
    "from sklearn.preprocessing import MinMaxScaler #normalisation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score #scoring\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation, colors\n",
    "from sklearn.datasets import load_iris\n",
    "```\n",
    "\n",
    "## Carregando Dados\n",
    "\n",
    "\n",
    "```python\n",
    "data_x, data_y = load_iris(return_X_y=True)\n",
    "train_x, test_x, train_y, test_y = train_test_split(data_x, data_y, test_size=0.2, random_state=42)\n",
    "```\n",
    "\n",
    "## Definições\n",
    "\n",
    "\n",
    "```python\n",
    "# Helper functions\n",
    "\n",
    "# Data Normalisation\n",
    "def minmax_scaler(data):\n",
    "  scaler = MinMaxScaler()\n",
    "  scaled = scaler.fit_transform(data)\n",
    "  return scaled\n",
    "\n",
    "# Euclidean distance\n",
    "def e_distance(x,y):\n",
    "  return distance.euclidean(x,y)\n",
    "\n",
    "# Manhattan distance\n",
    "def m_distance(x,y):\n",
    "  return distance.cityblock(x,y)\n",
    "\n",
    "# Best Matching Unit search\n",
    "def winning_neuron(data, t, som, num_rows, num_cols):\n",
    "  winner = [0,0]\n",
    "  shortest_distance = np.sqrt(data.shape[1]) # initialise with max distance\n",
    "  for row in range(num_rows):\n",
    "    for col in range(num_cols):\n",
    "      distance = e_distance(som[row][col], data[t])\n",
    "      if distance < shortest_distance:\n",
    "        shortest_distance = distance\n",
    "        winner = [row,col]\n",
    "  return winner\n",
    "\n",
    "# Learning rate and neighbourhood range calculation\n",
    "def decay(step, max_steps,max_learning_rate,max_m_dsitance):\n",
    "  coefficient = 1.0 - (np.float64(step)/max_steps)\n",
    "  learning_rate = coefficient*max_learning_rate\n",
    "  neighbourhood_range = ceil(coefficient * max_m_dsitance)\n",
    "  return learning_rate, neighbourhood_range\n",
    "```\n",
    "\n",
    "\n",
    "```python\n",
    "def train(train_x, map_size, max_learning_rate, n_iterations, sigma_0, show_progress_each=None):\n",
    "  num_rows = num_cols = map_size\n",
    "  train_x_norm = minmax_scaler(train_x) # normalisation\n",
    "\n",
    "  # initialising self-organising map\n",
    "  num_dims = train_x_norm.shape[1] # numnber of dimensions in the input data\n",
    "  np.random.seed(40)\n",
    "  som = np.random.random_sample(size=(num_rows, num_cols, num_dims)) # map construction\n",
    "\n",
    "  # start training iterations\n",
    "  for step in range(n_iterations):\n",
    "    if (show_progress_each is not None) and ((step+1) % show_progress_each == 0):\n",
    "      print(\"Iteration: \", step+1) # print out the current iteration for every 1k\n",
    "    learning_rate, neighbourhood_range = decay(step, n_iterations, max_learning_rate, sigma_0)\n",
    "\n",
    "    t = np.random.randint(0,high=train_x_norm.shape[0]) # random index of traing data\n",
    "    winner = winning_neuron(train_x_norm, t, som, num_rows, num_cols)\n",
    "    for row in range(num_rows):\n",
    "      for col in range(num_cols):\n",
    "        if m_distance([row,col],winner) <= neighbourhood_range:\n",
    "          som[row][col] += learning_rate*(train_x_norm[t]-som[row][col]) #update neighbour's weight\n",
    "\n",
    "  return som, train_x_norm\n",
    "```\n",
    "\n",
    "\n",
    "```python\n",
    "# collecting labels\n",
    "\n",
    "def collect_labels(train_y, map_size, train_x_norm, som):\n",
    "  num_rows = num_cols = map_size\n",
    "  label_data = train_y\n",
    "  map = np.empty(shape=(num_rows, num_cols), dtype=object)\n",
    "\n",
    "  for row in range(num_rows):\n",
    "    for col in range(num_cols):\n",
    "      map[row][col] = [] # empty list to store the label\n",
    "\n",
    "  for t in range(train_x_norm.shape[0]):\n",
    "    if (t+1) % 1000 == 0:\n",
    "      print(\"sample data: \", t+1)\n",
    "    winner = winning_neuron(train_x_norm, t, som, num_rows, num_cols)\n",
    "    map[winner[0]][winner[1]].append(label_data[t]) # label of winning neuron\n",
    "  \n",
    "  return map\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "```python\n",
    "# construct label map\n",
    "\n",
    "def get_label_map(map_size, map,):\n",
    "  num_rows = num_cols = map_size\n",
    "  label_map = np.zeros(shape=(num_rows, num_cols),dtype=np.int64)\n",
    "  for row in range(num_rows):\n",
    "    for col in range(num_cols):\n",
    "      label_list = map[row][col]\n",
    "      if len(label_list)==0:\n",
    "        label = 2\n",
    "      else:\n",
    "        label = max(label_list, key=label_list.count)\n",
    "      label_map[row][col] = label\n",
    "  return label_map\n",
    "\n",
    "def show_label_maps(label_maps, titles):\n",
    "  n = len(label_maps)\n",
    "  fig, axes = plt.subplots(1, n, figsize=(5 * n, 5))\n",
    "  if n == 1:\n",
    "    axes = [axes]\n",
    "  cmap = colors.ListedColormap(['tab:green', 'tab:red', 'tab:orange'])\n",
    "  for i, (label_map, title) in enumerate(zip(label_maps, titles)):\n",
    "    ax = axes[i]\n",
    "    im = ax.imshow(label_map, cmap=cmap)\n",
    "    ax.set_title(title)\n",
    "    # Annotate each cell with its label value\n",
    "    for (row, col), label in np.ndenumerate(label_map):\n",
    "      ax.text(col, row, str(label), ha='center', va='center', color='black', fontsize=12)\n",
    "    fig.colorbar(im, ax=ax)\n",
    "  plt.tight_layout()\n",
    "  plt.show()\n",
    "```\n",
    "\n",
    "\n",
    "```python\n",
    "def accuracy(test_x, som, map_size, label_map):\n",
    "    num_rows = num_cols = map_size\n",
    "    data = minmax_scaler(test_x) # normalisation\n",
    "\n",
    "    winner_labels = []\n",
    "\n",
    "    for t in range(data.shape[0]):\n",
    "        winner = winning_neuron(data, t, som, num_rows, num_cols)\n",
    "        row = winner[0]\n",
    "        col = winner[1]\n",
    "        predicted = label_map[row][col]\n",
    "        winner_labels.append(predicted)\n",
    "\n",
    "    accu = accuracy_score(test_y, np.array(winner_labels))\n",
    "    return accu\n",
    "```\n",
    "\n",
    "\n",
    "```python\n",
    "def execute_experiment(scenarios):\n",
    "    label_maps = []\n",
    "    titles = []\n",
    "\n",
    "    for scenario in scenarios:\n",
    "        map_size = scenario['map_size']\n",
    "        max_learning_rate = scenario['max_learning_rate']\n",
    "        n_iterations = scenario['n_iterations']\n",
    "        sigma_0 = scenario['sigma_0']\n",
    "\n",
    "        som, train_x_norm = train(train_x, map_size, max_learning_rate, n_iterations, sigma_0)\n",
    "        map = collect_labels(train_y, map_size, train_x_norm, som)\n",
    "        label_map = get_label_map(map_size, map)\n",
    "        accu = accuracy(test_x, som, map_size, label_map)\n",
    "\n",
    "        label_maps.append(label_map)\n",
    "\n",
    "        map_size_title = rf'${map_size}x{map_size}$'\n",
    "        learning_rate_title = rf'$\\alpha(t)={max_learning_rate}$'\n",
    "        iterations_title = rf'$n={n_iterations}$'\n",
    "        sigma_title = rf'$\\sigma_0={sigma_0}$'\n",
    "        accuracy_title = rf'accuracy: {int(accu*100)}%'\n",
    "\n",
    "        titles.append(f'{map_size_title} | {learning_rate_title} | {iterations_title} | {sigma_title}\\n{accuracy_title}')\n",
    "    \n",
    "    show_label_maps(label_maps, titles)\n",
    "```\n",
    "\n",
    "## Experimentos\n",
    "\n",
    "Aproveitando a sugestão feita no slide de tutorial do trabalho, me basearei no mesmo para rodar os experimentos.\n",
    "\n",
    "Em cada experimento, irei variar os seguintes parâmetros:\n",
    "- Tamanho do mapa (map_size)\n",
    "- Taxa de aprendizado (max_learning_rate)\n",
    "- Número de iterações (n_iterations)\n",
    "- Sigma inicial (sigma_0)\n",
    "\n",
    "Um por vez. Por exemplo, no experimento 1, alterarei o tamanho do mapa para ver sua influência e fixarei os demais parâmetros.\n",
    "\n",
    "Os valores que os mesmos assumirão serão:\n",
    "\n",
    "| Parâmetro          | Valores         |\n",
    "|--------------------|---------------|\n",
    "| Tamanho do Mapa    | 5x5, 10x10, 15x15 |\n",
    "| Taxa de Aprendizado | 0.5, 0.1, 0.01 |\n",
    "| Número de Iterações | 1000, 5000, 10000, 30000, 50000 |\n",
    "| Sigma Inicial      | 3, 5, 7 |\n",
    "\n",
    "### Experimento 1: Tamanho do Mapa\n",
    "\n",
    "\n",
    "```python\n",
    "map_size_experiment = [\n",
    "    {'map_size': 5, 'max_learning_rate': 0.5, 'n_iterations': 1000, 'sigma_0': 3},\n",
    "    {'map_size': 10, 'max_learning_rate': 0.5, 'n_iterations': 1000, 'sigma_0': 3},\n",
    "    {'map_size': 15, 'max_learning_rate': 0.5, 'n_iterations': 1000, 'sigma_0': 3}\n",
    "]\n",
    "execute_experiment(map_size_experiment)\n",
    "```\n",
    "\n",
    "\n",
    "    \n",
    "![png](Avaliacao2Kohonen_files/Avaliacao2Kohonen_14_0.png)\n",
    "    \n",
    "\n",
    "\n",
    "Nesse experimento, com um aumento do tamanho do mapa, é perceptível que a acurácia diminui e o agrupamento fica cada vez pior. Dentre as alterações, o tamanho $5\\times5$ foi o que obteve a melhor acurácia, com $96\\%$.\n",
    "\n",
    "### Experimento 2: Taxa de Aprendizado\n",
    "\n",
    "\n",
    "```python\n",
    "max_learning_rate_experiment = [\n",
    "    {'map_size': 5, 'max_learning_rate': 0.5, 'n_iterations': 1000, 'sigma_0': 3},\n",
    "    {'map_size': 5, 'max_learning_rate': 0.1, 'n_iterations': 1000, 'sigma_0': 3},\n",
    "    {'map_size': 5, 'max_learning_rate': 0.01, 'n_iterations': 1000, 'sigma_0': 3}\n",
    "]\n",
    "execute_experiment(max_learning_rate_experiment)\n",
    "```\n",
    "\n",
    "\n",
    "    \n",
    "![png](Avaliacao2Kohonen_files/Avaliacao2Kohonen_17_0.png)\n",
    "    \n",
    "\n",
    "\n",
    "Nesse experimento, a acuracia cai no segundo cenário, mas volta para $96\\%$ no terceiro cenário. Essa queda é de apenas $3\\%$, então não é tão preocupante. E pode-se perceber que os agrupamentos entre os mapas são bem próximos.\n",
    "\n",
    "### Experimento 3: Número de Iterações\n",
    "\n",
    "\n",
    "```python\n",
    "n_iterations_experiment = [\n",
    "    {'map_size': 5, 'max_learning_rate': 0.5, 'n_iterations': 1000, 'sigma_0': 3},\n",
    "    {'map_size': 5, 'max_learning_rate': 0.5, 'n_iterations': 5000, 'sigma_0': 3},\n",
    "    {'map_size': 5, 'max_learning_rate': 0.5, 'n_iterations': 10_000, 'sigma_0': 3},\n",
    "    {'map_size': 5, 'max_learning_rate': 0.5, 'n_iterations': 30_000, 'sigma_0': 3},\n",
    "    {'map_size': 5, 'max_learning_rate': 0.5, 'n_iterations': 50_000, 'sigma_0': 3}\n",
    "]\n",
    "execute_experiment(n_iterations_experiment)\n",
    "```\n",
    "\n",
    "\n",
    "    \n",
    "![png](Avaliacao2Kohonen_files/Avaliacao2Kohonen_20_0.png)\n",
    "    \n",
    "\n",
    "\n",
    "Nesse experimento, houve uma variabilidade de mapas bem maior, mas o resultado mais interessante foi que a acurácia de $100\\%$ para $30.000$ iterações, o que mostra que o número de iterações é um fator importante para a acurácia do modelo.\n",
    "\n",
    "### Experimento 4: Sigma Inicial\n",
    "\n",
    "\n",
    "```python\n",
    "sigma_0_experiment = [\n",
    "    {'map_size': 5, 'max_learning_rate': 0.5, 'n_iterations': 1000, 'sigma_0': 3},\n",
    "    {'map_size': 5, 'max_learning_rate': 0.5, 'n_iterations': 1000, 'sigma_0': 5},\n",
    "    {'map_size': 5, 'max_learning_rate': 0.5, 'n_iterations': 1000, 'sigma_0': 7}\n",
    "]\n",
    "execute_experiment(sigma_0_experiment)\n",
    "```\n",
    "\n",
    "\n",
    "    \n",
    "![png](Avaliacao2Kohonen_files/Avaliacao2Kohonen_23_0.png)\n",
    "    \n",
    "\n",
    "\n",
    "Nesse experimento, o aumento do sigma inicial abaixa a acurácia do modelo, mas, assim como os outros experimentos, não é uma queda tão grande. A acurácia foi de $96\\%$ para $93\\%$.\n",
    "\n",
    "## Conclusão\n",
    "\n",
    "Todas as alterações tem impacto na acurácia do modelo, mas o número de iterações e o tamanho do mapa são os que mais influenciaram para esses experimentos. Mais especificamente, aumentando o número de iterações para ajudar o modelo a aprender melhor e diminuindo o tamanho do mapa para que ele não fique tão grande e disperso, o que ajuda a manter os agrupamentos mais próximos.\n",
    "\n",
    "Essa última afirmação sobre o tamanho do mapa, é claro, foi pensada para esses cenários de experimento. Não quer dizer que o menor mapa sempre será melhor, mas sim que, para esses dados, o tamanho $5\\times5$ foi o melhor e ele é o menor dentre os testados.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
